# HICom

The official implementation of [Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models](https://arxiv.org/abs/2503.16036) (CVPR 2025)



<h5 align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2503.16036-AD1C18.svg?logo=arXiv)](https://arxiv.org/abs/2503.16036)
[![hf_checkpoint](https://img.shields.io/badge/ğŸ¤—-Checkpoints-9C276A.svg)](https://huggingface.co/lntzm/HICom_7B_qwen25_directg_local43_global32)
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](https://github.com/lntzm/HICom/blob/main/LICENSE) 
</h5>

## ğŸ› ï¸ Environment Preparation
```bash
git clone https://github.com/lntzm/HICom.git
cd HICom
conda create -n hicom python==3.10
conda activate hicom
conda install pytorch==2.4.1 torchvision==0.19.1 pytorch-cuda=12.1 -c pytorch -c nvidia
pip install numpy==1.26.4
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

## ğŸ“œ Data Preparation
We put all our training and evaluation model under `playground` folder. The structure are here:
```
playground
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ eval_image -> /.../LLaVA/playground/data/eval # Link LLaVA eval folder here
â”‚   â”œâ”€â”€ eval_video
â”‚   â”‚   â”œâ”€â”€ Activitynet_Zero_Shot_QA
â”‚   â”‚   â”œâ”€â”€ EgoSchema
â”‚   â”‚   â”œâ”€â”€ MLVU
â”‚   â”‚   â”œâ”€â”€ MSRVTT_Zero_Shot_QA
â”‚   â”‚   â”œâ”€â”€ MSVD_Zero_Shot_QA
â”‚   â”‚   â”œâ”€â”€ MVBench
â”‚   â”‚   â”œâ”€â”€ Video-ChatGPT-eval
â”‚   â”‚   â””â”€â”€ Video-MME
â”‚   â”œâ”€â”€ Ins-VL
â”‚   â”œâ”€â”€ LLaVA-Instruct-150K
â”‚   â”œâ”€â”€ LLaVA-Pretrain
â”‚   â””â”€â”€ Video_Mix_Instruct
â”‚       â”œâ”€â”€ Charades
â”‚       â”œâ”€â”€ CLEVER
â”‚       â”œâ”€â”€ LLaVA-Hound
â”‚       â”œâ”€â”€ LLaVA-Video-178K
â”‚       â”œâ”€â”€ m4_instruct_videos
â”‚       â”œâ”€â”€ mit_action
â”‚       â”œâ”€â”€ NTU-RGB-D
â”‚       â”œâ”€â”€ ssv2-cls
â”‚       â”œâ”€â”€ TVQA
â”‚       â””â”€â”€ Video-ChatGPT-0525
â””â”€â”€ models
    â”œâ”€â”€ Qwen2.5-0.5B-Instruct
    â”œâ”€â”€ Qwen2.5-1.5B-Instruct
    â”œâ”€â”€ Qwen2.5-7B-Instruct
    â””â”€â”€ siglip-so400m-patch14-384
```

## ğŸ’° Train
Train scripts are under `scripts/qwen2.5_7B` folder.
```bash
bash scripts/qwen2.5_7B/release/directg_local43_global32.sh
```

## ğŸ¤— Checkpoints
We release our trained checkpoint in [Huggingface](https://huggingface.co/lntzm/HICom_7B_qwen25_directg_local43_global32), which performs a little higher than reported, as we re-organize the code, fix some bugs, upgrade the environment, and re-train the model with unfreezing the text encoder.

## ğŸ¤– Evaluation
video evaluation scripts are under `scripts/eval/video` folder.
```bash
# videomme
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/eval/video/eval_video_mcqa_videomme.sh CKPT_PATH

# mvbench
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/eval/video/eval_video_mcqa_mvbench.sh CKPT_PATH

# egoschema
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/eval/video/eval_video_mcqa_egoschema.sh CKPT_PATH

# mlvu
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/eval/video/eval_video_mcqa_mlvu.sh CKPT_PATH
```

## ğŸ“‘ Citation

If you find our work useful for your research and applications, please cite using this BibTeX:
```bibtex
@article{liu2025hybrid,
  title={Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models},
  author={Liu, Zhihang and Xie, Chen-Wei and Li, Pandeng and Zhao, Liming and Tang, Longxiang and Zheng, Yun and Liu, Chuanbin and Xie, Hongtao},
  journal={arXiv preprint arXiv:2503.16036},
  year={2025}
}
```

## ğŸ‘ Acknowledgement
The codebase of HICom is adapted from [**VideoLLaMA 2**](https://github.com/DAMO-NLP-SG/VideoLLaMA2) and [**LLaVA-OneVision**](https://github.com/LLaVA-VL/LLaVA-NeXT). We are also grateful for the following projects our HICom arise from:
[**Qwen2.5**](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e), [**SigLIP**](https://huggingface.co/collections/google/siglip-659d5e62f0ae1a57ae0e83ba), [**Panda-70M**](https://github.com/snap-research/Panda-70M).


## ğŸ”’ License

This project is released under the Apache 2.0 license as found in the LICENSE file.
The service is a research preview intended for **non-commercial use ONLY**, subject to the model Licenses of LLaMA and Mistral, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please get in touch with us if you find any potential violations.
